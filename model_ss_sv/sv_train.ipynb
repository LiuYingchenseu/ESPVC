{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, StratifiedGroupKFold, GroupKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import seaborn as sns\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, accuracy_score, recall_score, f1_score, precision_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import joblib\n",
    "\n",
    "os.chdir('..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df = pd.read_csv('opensmile.csv')\n",
    "# df = pd.read_csv('opensmile_standardized.csv')\n",
    "# # df = shuffle(df)\n",
    "# # df.reset_index(inplace=True, drop=True)\n",
    "# # df.drop('voiceID', inplace = True, axis = 1)\n",
    "# df['label'].value_counts()\n",
    "# df['gender'].value_counts()\n",
    "# df['age'].value_counts()\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = df.iloc[:, 4:].values  # feather\n",
    "# y = df['label'].values  # label\n",
    "# speakers = df['speaker'].values  # Speaker information\n",
    "\n",
    "# kf = StratifiedGroupKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# X_train_folds = []\n",
    "# Y_train_folds = []\n",
    "# X_test_folds = []\n",
    "# Y_test_folds = []\n",
    "# speaker_train_folds = []  # Used to store the speaker corresponding to each training fold\n",
    "# speaker_test_folds = []  # Used to store the speaker corresponding to each test fold\n",
    "\n",
    "# for train_index, test_index in kf.split(x, y, speakers):\n",
    "#     X_train_folds.append(x[train_index])\n",
    "#     Y_train_folds.append(y[train_index])\n",
    "#     X_test_folds.append(x[test_index])\n",
    "#     Y_test_folds.append(y[test_index])\n",
    "    \n",
    "#     # Save the speaker information for each compromise\n",
    "#     speaker_train_folds.append(speakers[train_index])\n",
    "#     speaker_test_folds.append(speakers[test_index])\n",
    "\n",
    "# # Save Data\n",
    "# for i in range(5):\n",
    "#     # Save the training and testing datase\n",
    "#     np.save(f'./X_train_{i}.npy', X_train_folds[i])\n",
    "#     np.save(f'./Y_train_{i}.npy', Y_train_folds[i])\n",
    "#     np.save(f'./X_test_{i}.npy', X_test_folds[i])\n",
    "#     np.save(f'./Y_test_{i}.npy', Y_test_folds[i])\n",
    "    \n",
    "#     # Save the speaker information for each fold\n",
    "#     # np.save(f'./speaker_train_{i}.npy', speaker_train_folds[i])\n",
    "#     # np.save(f'./speaker_test_{i}.npy', speaker_test_folds[i])\n",
    "#     # Save the speaker information for each fold to a txt file\n",
    "#     with open(f'./speaker_train_{i}.txt', 'w') as f_train:\n",
    "#         for speaker in speaker_train_folds[i]:\n",
    "#             f_train.write(f'{speaker}\\n')  # Write one line for each speaker's information\n",
    "    \n",
    "#     with open(f'./speaker_test_{i}.txt', 'w') as f_test:\n",
    "#         for speaker in speaker_test_folds[i]:\n",
    "#             f_test.write(f'{speaker}\\n')  # Write one line for each speaker's information\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('./X_all_train_mutual.npy')\n",
    "Y_train = np.load('./Y_train.npy')\n",
    "X_test = np.load('./X_all_test_mutual.npy')\n",
    "Y_test = np.load('./Y_test.npy')\n",
    "\n",
    "X_train, Y_train = shuffle(X_train, Y_train, random_state=42)\n",
    "X_test, Y_test = shuffle(X_test, Y_test, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scale\n",
    "# sc = StandardScaler()\n",
    "\n",
    "# # sc = MinMaxScaler()\n",
    "# # sc = joblib.load(scaler.pkl\")\n",
    "\n",
    "# # sc = RobustScaler()\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# X_test = sc.transform(X_test)\n",
    "\n",
    "# # Save data scale parameters\n",
    "# joblib.dump(sc, \"scaler.pkl\")\n",
    "\n",
    "# # pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "leaf_size = list(range(1, 50))\n",
    "n_neighbors = list(range(1, 20))\n",
    "p = [1, 2]\n",
    "knn_hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
    "\n",
    "# Define KNN model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Use the existing cross-validation (do not set cv again in GridSearchCV)\n",
    "knn_clf = GridSearchCV(knn, knn_hyperparameters)  # Remove cv=10\n",
    "\n",
    "# fitted model\n",
    "knn_best_model = knn_clf.fit(X_train, Y_train)\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(knn_best_model, \"./knn.pkl\")\n",
    "\n",
    "# Output the optimal hyperparameters and scores\n",
    "print('Best leaf_size:       ', knn_best_model.best_estimator_.get_params()['leaf_size'])\n",
    "print('Best p:               ', knn_best_model.best_estimator_.get_params()['p'])\n",
    "print('Best n_neighbors:     ', knn_best_model.best_estimator_.get_params()['n_neighbors'])\n",
    "print('Best Score:           %s' % knn_best_model.best_score_)\n",
    "print('Best Hyperparameters: %s' % knn_best_model.best_params_)\n",
    "\n",
    "# Prediction test set\n",
    "y_pred_knn = knn_best_model.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix and performance metrics\n",
    "knn_conf_matrix = confusion_matrix(Y_test, y_pred_knn)\n",
    "tn, fp, fn, tp = knn_conf_matrix.ravel()\n",
    "knn_sen = tp / (tp + fn)\n",
    "knn_spe = tn / (tn + fp)\n",
    "\n",
    "# result\n",
    "print(f\"ConfusionMatrix: \\n {knn_conf_matrix}\")      \n",
    "print('Accuracy: ', accuracy_score(Y_test, y_pred_knn))  # Accuracy\n",
    "print(f'Specificity: {knn_spe}')  # Specificity\n",
    "print(f'Sensitivity:  {knn_sen}')  # Sensitivity\n",
    "print('F1-score: ', f1_score(Y_test, y_pred_knn, average='binary'))  # F1-score\n",
    "print('Recall: ', recall_score(Y_test, y_pred_knn, average='binary'))  # Recall\n",
    "print('Precision: ', precision_score(Y_test, y_pred_knn, average='binary'))  # Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "max_depth = list(range(1, 10))\n",
    "min_samples_split = list(range(2, 10))\n",
    "min_samples_leaf = list(range(1, 5))\n",
    "criterion = ['gini', 'entropy']\n",
    "dt_hyperparameters = dict(max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, criterion=criterion)\n",
    "\n",
    "# Define decision tree model\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "\n",
    "# Using GridSearchCV for hyperparameter selection does not require specifying the cv parameter\n",
    "dt_clf = GridSearchCV(dt, dt_hyperparameters, refit=True)  # Remove cv=10\n",
    "\n",
    "# fitted model\n",
    "dt_best_model = dt_clf.fit(X_train, Y_train)\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(dt_best_model, \"dt.pkl\")\n",
    "\n",
    "# Output the optimal hyperparameters and scores\n",
    "print('Best Score:           %s' % dt_best_model.best_score_)\n",
    "print('Best Hyperparameters: %s' % dt_best_model.best_params_)\n",
    "\n",
    "# Prediction test set\n",
    "y_pred_dt = dt_best_model.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix and performance metrics\n",
    "dt_conf_matrix = confusion_matrix(Y_test, y_pred_dt)\n",
    "tn, fp, fn, tp = dt_conf_matrix.ravel()\n",
    "dt_sen = tp / (tp + fn)\n",
    "dt_spe = tn / (tn + fp)\n",
    "\n",
    "# result\n",
    "print(f\"ConfusionMatrix: \\n {dt_conf_matrix}\")      \n",
    "print('Accuracy:  ', accuracy_score(Y_test, y_pred_dt))  # Accuracy\n",
    "print(f'Specificity: {dt_spe}')  # Specificity\n",
    "print(f'Sensitivity:  {dt_sen}')  # Sensitivity\n",
    "print('F1-score: ', f1_score(Y_test, y_pred_dt, average='binary'))  # F1-score\n",
    "print('Recall: ', recall_score(Y_test, y_pred_dt, average='binary'))  # Recall\n",
    "print('Precision: ', precision_score(Y_test, y_pred_dt, average='binary'))  # Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "C = [0.1, 1, 10, 100, 1000]\n",
    "gamma = [1, 0.1, 0.01, 0.001, 0.0001]\n",
    "# rbf -> Gaussian kernel, poly -> polynomial kernel, linear -> linear kernel, sigmoid -> sigmoid kernel\n",
    "kernel = ['rbf', 'poly', 'linear', 'sigmoid']\n",
    "svm_hyperparameters = dict(C=C, gamma=gamma, kernel=kernel)\n",
    "\n",
    "# Define SVM model\n",
    "svm2 = svm.SVC(probability=True)\n",
    "\n",
    "# When using GridSearchCV for hyperparameter selection, there is no need to specify the cv parameter\n",
    "svm_clf = GridSearchCV(svm2, svm_hyperparameters, refit=True)  # Remove cv=10\n",
    "\n",
    "# fitted model\n",
    "svm_best_model = svm_clf.fit(X_train, Y_train)\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(svm_best_model, \"svm.pkl\")\n",
    "\n",
    "# Output the optimal hyperparameters and scores\n",
    "print('Best Score:           %s' % svm_best_model.best_score_)\n",
    "print('Best Hyperparameters: %s' % svm_best_model.best_params_)\n",
    "\n",
    "# Prediction test set\n",
    "y_pred_svm = svm_best_model.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix and performance metrics\n",
    "svm_conf_matrix = confusion_matrix(Y_test, y_pred_svm)\n",
    "tn, fp, fn, tp = svm_conf_matrix.ravel()\n",
    "svm_sen = tp / (tp + fn)\n",
    "svm_spe = tn / (tn + fp)\n",
    "\n",
    "# result\n",
    "print(f\"ConfusionMatrix: \\n {svm_conf_matrix}\")      \n",
    "print('Accuracy:  ', accuracy_score(Y_test, y_pred_svm))  # Accuracy\n",
    "print(f'Specificity: {svm_spe}')  # Specificity\n",
    "print(f'Sensitivity:  {svm_sen}')  # Sensitivity\n",
    "print('F1-score: ', f1_score(Y_test, y_pred_svm, average='binary'))  # F1-score\n",
    "print('Recall: ', recall_score(Y_test, y_pred_svm, average='binary'))  # Recall\n",
    "print('Precision: ', precision_score(Y_test, y_pred_svm, average='binary'))  # Precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters tuning for AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200]\n",
    "learning_rate = [1, 0.5, 0.25, 0.1, 0.05, 0.01]\n",
    "abc_hyperparameters = dict(n_estimators=n_estimators, learning_rate=learning_rate)\n",
    "\n",
    "# Define AdaBoost model\n",
    "abc_model = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=1))\n",
    "\n",
    "# Use GridSearchCV for hyperparameter selection, but cv parameter is no longer needed\n",
    "abc_clf = GridSearchCV(abc_model, abc_hyperparameters, refit=True)  # Remove cv parameters\n",
    "\n",
    "# fitted model\n",
    "abc_best_model = abc_clf.fit(X_train, Y_train)\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(abc_best_model, \"ab.pkl\")\n",
    "\n",
    "# Output the optimal hyperparameters and scores\n",
    "print('Best Score:           %s' % abc_best_model.best_score_)\n",
    "print('Best Hyperparameters: %s' % abc_best_model.best_params_)\n",
    "\n",
    "# Prediction test set\n",
    "y_pred_abc = abc_best_model.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix and performance metrics\n",
    "abc_conf_matrix = confusion_matrix(Y_test, y_pred_abc)\n",
    "tn, fp, fn, tp = abc_conf_matrix.ravel()\n",
    "abc_sen = tp / (tp + fn)\n",
    "abc_spe = tn / (tn + fp)\n",
    "\n",
    "# result\n",
    "print(f\"ConfusionMatrix: \\n {abc_conf_matrix}\")      \n",
    "print('Accuracy:  ', accuracy_score(Y_test, y_pred_abc))  # Accuracy\n",
    "print(f'Specificity: {abc_spe}')  # Specificity\n",
    "print(f'Sensitivity:  {abc_sen}')  # Sensitivity\n",
    "print('Recall: ', recall_score(Y_test, y_pred_abc, average='binary'))  # Recall\n",
    "print('F1-score: ', f1_score(Y_test, y_pred_abc, average='binary'))  # F1-score\n",
    "print('Precision: ', precision_score(Y_test, y_pred_abc, average='binary'))  # Precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters tuning for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200]\n",
    "max_depth = [1, 3, 5, 7, 9]\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 5, 10, 15]\n",
    "\n",
    "rf_hyperparameters = dict(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features, \n",
    "                          min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "# Create a random forest classifier\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "# Use GridSearchCV for hyperparameter selection and remove the cv parameter\n",
    "rf_clf = GridSearchCV(rf_model, rf_hyperparameters, refit=True)  # Remove cv parameter\n",
    "\n",
    "# fitted model\n",
    "rf_best_model = rf_clf.fit(X_train, Y_train)\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(rf_best_model, \"rf.pkl\")\n",
    "\n",
    "# Output the optimal hyperparameters and scores\n",
    "print('Best Score:           %s' % rf_best_model.best_score_)\n",
    "print('Best Hyperparameters: %s' % rf_best_model.best_params_)\n",
    "\n",
    "# Prediction test set\n",
    "y_pred_rf = rf_best_model.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix and performance metrics\n",
    "rf_conf_matrix = confusion_matrix(Y_test, y_pred_rf)\n",
    "tn, fp, fn, tp = rf_conf_matrix.ravel()\n",
    "rf_sen = tp / (tp + fn)\n",
    "rf_spe = tn / (tn + fp)\n",
    "\n",
    "# result\n",
    "print(f\"ConfusionMatrix: \\n {rf_conf_matrix}\")      \n",
    "print('Accuracy:  ', accuracy_score(Y_test, y_pred_rf))  # Accuracy\n",
    "print(f'Specificity: {rf_spe}')  # Specificity\n",
    "print(f'Sensitivity:  {rf_sen}')  # Sensitivity\n",
    "print('Recall: ', recall_score(Y_test, y_pred_rf, average='binary'))  # Recall\n",
    "print('F1-score: ', f1_score(Y_test, y_pred_rf, average='binary'))  # F1-score\n",
    "print('Precision: ', precision_score(Y_test, y_pred_rf, average='binary'))  # Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Set hyperparameters\n",
    "n_estimators = [50, 100, 200, 300]\n",
    "learning_rate = [0.1, 0.01, 0.05, 0.3]\n",
    "max_depth = [3, 6, 9, 12]\n",
    "subsample = [0.7, 0.8, 0.9, 1.0]\n",
    "colsample_bytree = [0.7, 0.8, 1.0]\n",
    "\n",
    "xgb_hyperparameters = dict(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth,\n",
    "                           subsample=subsample, colsample_bytree=colsample_bytree)\n",
    "\n",
    "# Create an XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "# Use GridSearchCV for hyperparameter selection and remove the cv parameter\n",
    "xgb_clf = GridSearchCV(xgb_model, xgb_hyperparameters, refit=True)\n",
    "\n",
    "# fitted model\n",
    "xgb_best_model = xgb_clf.fit(X_train, Y_train)\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(xgb_best_model, \"xgb.pkl\")\n",
    "\n",
    "# Output the optimal hyperparameters and scores\n",
    "print('Best Score:           %s' % xgb_best_model.best_score_)\n",
    "print('Best Hyperparameters: %s' % xgb_best_model.best_params_)\n",
    "\n",
    "# Prediction test set\n",
    "y_pred_xgb = xgb_best_model.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix and performance metrics\n",
    "xgb_conf_matrix = confusion_matrix(Y_test, y_pred_xgb)\n",
    "tn, fp, fn, tp = xgb_conf_matrix.ravel()\n",
    "xgb_sen = tp / (tp + fn)\n",
    "xgb_spe = tn / (tn + fp)\n",
    "\n",
    "# result\n",
    "print(f\"ConfusionMatrix: \\n {xgb_conf_matrix}\")      \n",
    "print('Accuracy:  ', accuracy_score(Y_test, y_pred_xgb))  # Accuracy\n",
    "print(f'Specificity: {xgb_spe}')  # Specificity\n",
    "print(f'Sensitivity:  {xgb_sen}')  # Sensitivity\n",
    "print('Recall: ', recall_score(Y_test, y_pred_xgb, average='binary'))  # Recall\n",
    "print('F1-score: ', f1_score(Y_test, y_pred_xgb, average='binary'))  # F1-score\n",
    "print('Precision: ', precision_score(Y_test, y_pred_xgb, average='binary'))  # Precision\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
